{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to NLP (aka) Text Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Intro](#intro)\n",
    "2. [Word Countings](#bow)\n",
    "    * [Bag of words](#bow)\n",
    "    * [Term Frequency - Inverse Document Frequency](#tf_idf)\n",
    "3. [Word Embeddings](#embeddings)\n",
    "    - [Word2Vec](#word2vec)\n",
    "    - [Glove](#glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NLP stands for **Natural Language Processings**. \n",
    "\n",
    "This is area of Machine Learning, where the data is a bunch of text / corpus / feedback reviews etc. ie., it deals with bunch of textual data and we have to identify the patterns or insights from them. \n",
    "\n",
    "For example, from the feedback reviews / movie comments, we can identify the **sentiment (good or bad)**, we can classify the what kind of **genre** (commedy, drama, etc). Also, can **summarize** the movie / article in few sentences as well.\n",
    "\n",
    "The **vocabulary** is the feature set for the NLP kind of applications.\n",
    "\n",
    "<hr width=100% size=4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use vocabulary for Machine Learning / NLP?\n",
    "\n",
    "The **vocabulary** is the feature set for NLP kind of applications. But, we can't feed the words to Machine Learning algorithms; they expect the numbers.\n",
    "\n",
    "So, the idea is to convert the **vocabulary** into bunch of numbers (based on their occurrence count in the document / some other means - vector space). The **number**  representation of the input vocabulary is called **embeddings** / **document matrix**.\n",
    "\n",
    "Once we get the ** number representation / embeddings **, we can **fit** them to any ML model.\n",
    "Note that to get the **embeddings**, we have to do **fit_transform()** method in the SciKit Learn.\n",
    "\n",
    "Also, note that, for the validation dataset, you have to **transform()** the words to **embeddings** before feeding to **predict()** method.\n",
    "\n",
    "So the flow is as follows:\n",
    "\n",
    "1.  Collect the vocabulary set. Use stopwords from nltk library to restrict the common used words.\n",
    "*  embeddings = vectorizer(vocabulary_set)\n",
    "*  model.fit_transform(embeddings)\n",
    "*  test_embeddings = vectorizer.transform(validation_vocab_set)\n",
    "*  pred = model.predict(test_embeddgins)\n",
    "*  Collect the metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr width=100% size=4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Word Countings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
